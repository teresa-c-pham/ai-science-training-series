{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b082df",
   "metadata": {},
   "source": [
    "## ExerciseConcurrency.ipynb\n",
    "Play with the value of `num_threads` and note how it affects run time and throughput. How does the return value of `multiprocessing.cpu_count()`, that is, how does the number of CPU cores on your machine impact the throughput you can achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622bc619",
   "metadata": {},
   "source": [
    "**ANSWER:** Increasing the num_threads increases throughput and decreases run time because more threads are running in parallel. The more CPU cores the machine has, the more throughput it produces because the machine can complete tasks faster (lower run time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a87aa",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b948e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import time\n",
    "\n",
    "# This limits the amount of memory used:\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_auto_jit=2\"\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93d91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, n_filters):\n",
    "        # tf.keras.Model.__init__(self)\n",
    "        super(ResidualLayer, self).__init__()\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters     = n_filters,\n",
    "            kernel_size = (3,3),\n",
    "            padding     = \"same\"\n",
    "        )\n",
    "\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters     = n_filters,\n",
    "            kernel_size = (3,3),\n",
    "            padding     = \"same\"\n",
    "        )\n",
    "\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = inputs\n",
    "\n",
    "        output1 = self.norm1(self.conv1(inputs))\n",
    "\n",
    "        output1 = tf.keras.activations.relu(output1)\n",
    "\n",
    "        output2 = self.norm2(self.conv2(output1))\n",
    "\n",
    "        return tf.keras.activations.relu(output2 + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260a1ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualDownsample(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, n_filters):\n",
    "        # tf.keras.Model.__init__(self)\n",
    "        super(ResidualDownsample, self).__init__()\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters     = n_filters,\n",
    "            kernel_size = (3,3),\n",
    "            padding     = \"same\",\n",
    "            strides     = (2,2)\n",
    "        )\n",
    "\n",
    "        self.identity = tf.keras.layers.Conv2D(\n",
    "            filters     = n_filters,\n",
    "            kernel_size = (1,1),\n",
    "            strides     = (2,2),\n",
    "            padding     = \"same\"\n",
    "        )\n",
    "\n",
    "        self.norm1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters     = n_filters,\n",
    "            kernel_size = (3,3),\n",
    "            padding     = \"same\"\n",
    "        )\n",
    "\n",
    "        self.norm2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.identity(inputs)\n",
    "        output1 = self.norm1(self.conv1(inputs))\n",
    "        output1 = tf.keras.activations.relu(output1)\n",
    "\n",
    "        output2 = self.norm2(self.conv2(output1))\n",
    "\n",
    "        return tf.keras.activations.relu(output2 + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f532ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet34(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResNet34, self).__init__()\n",
    "\n",
    "        self.conv_init = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters     = 64,\n",
    "                kernel_size = (7,7),\n",
    "                strides     = (2,2),\n",
    "                padding     = \"same\",\n",
    "                use_bias    = False\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=\"same\")\n",
    "\n",
    "        ])\n",
    "\n",
    "        self.residual_series_1 = tf.keras.Sequential([\n",
    "            ResidualLayer(64),\n",
    "            ResidualLayer(64),\n",
    "            ResidualLayer(64),\n",
    "        ])\n",
    "\n",
    "        # Increase the number of filters:\n",
    "        self.downsample_1 = ResidualDownsample(128)\n",
    "\n",
    "        self.residual_series_2 = tf.keras.Sequential([\n",
    "            ResidualLayer(128),\n",
    "            ResidualLayer(128),\n",
    "            ResidualLayer(128),\n",
    "        ])\n",
    "\n",
    "        # Increase the number of filters:\n",
    "        self.downsample_2 = ResidualDownsample(256)\n",
    "\n",
    "        self.residual_series_3 = tf.keras.Sequential([\n",
    "            ResidualLayer(256),\n",
    "            ResidualLayer(256),\n",
    "            ResidualLayer(256),\n",
    "            ResidualLayer(256),\n",
    "            ResidualLayer(256),\n",
    "        ])\n",
    "\n",
    "        # Increase the number of filters:\n",
    "        self.downsample_3 = ResidualDownsample(512)\n",
    "\n",
    "\n",
    "        self.residual_series_4 = tf.keras.Sequential([\n",
    "            ResidualLayer(512),\n",
    "            ResidualLayer(512),\n",
    "        ])\n",
    "\n",
    "        self.final_pool = tf.keras.layers.AveragePooling2D(\n",
    "            pool_size=(8,8)\n",
    "        )\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.classifier = tf.keras.layers.Dense(1000)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.conv_init(inputs)\n",
    "\n",
    "        x = self.residual_series_1(x)\n",
    "\n",
    "\n",
    "        x = self.downsample_1(x)\n",
    "\n",
    "\n",
    "        x = self.residual_series_2(x)\n",
    "\n",
    "        x = self.downsample_2(x)\n",
    "\n",
    "        x = self.residual_series_3(x)\n",
    "\n",
    "        x = self.downsample_3(x)\n",
    "\n",
    "\n",
    "        x = self.residual_series_4(x)\n",
    "\n",
    "\n",
    "        x = self.final_pool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6fa1920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters set, preparing dataloading\n",
      "{\n",
      "    \"data\": {\n",
      "        \"handler\": \"ilsvrc_dataset\",\n",
      "        \"batch_size\": 128,\n",
      "        \"train_filelist\": \"/lus/grand/projects/ALCFAITP/ilsvrc_train_filelist.txt\",\n",
      "        \"test_filelist\": \"/lus/grand/projects/ALCFAITP/ilsvrc_val_filelist.txt\",\n",
      "        \"shuffle_buffer\": 200000,\n",
      "        \"reshuffle_each_iteration\": true,\n",
      "        \"num_parallel_readers\": 32,\n",
      "        \"prefetch_buffer_size\": 10,\n",
      "        \"crop_image_size\": [\n",
      "            256,\n",
      "            256\n",
      "        ],\n",
      "        \"num_classes\": 1000,\n",
      "        \"num_channels\": 3\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14692/457061918.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14692/457061918.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0mN_EPOCHS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_data_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14692/457061918.py\u001b[0m in \u001b[0;36mprepare_data_loader\u001b[1;34m(BATCH_SIZE)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Datasets ready, creating network.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;31m#########################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\extraML\\ai-science-training-series\\05_dataPipelines\\ilsvrc_dataset.py\u001b[0m in \u001b[0;36mget_datasets\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     46\u001b[0m    \u001b[0mtest_filelist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_filelist'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m    \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_filelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m    \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_filelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def calculate_accuracy(logits, labels):\n",
    "    # We calculate top1 accuracy only here:\n",
    "    selected_class = tf.argmax(logits, axis=1)\n",
    "\n",
    "    correct = tf.cast(selected_class, tf.float32) == tf.cast(labels, tf.float32)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def calculate_loss(logits, labels):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "@tf.function\n",
    "def training_step(network, optimizer, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = network(images)\n",
    "        loss = calculate_loss(logits, labels)\n",
    "\n",
    "    gradients = tape.gradient(loss, network.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, network.trainable_variables))\n",
    "\n",
    "    accuracy = calculate_accuracy(logits, labels)\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "def train_epoch(i_epoch, step_in_epoch, train_ds, val_ds, network, optimizer, BATCH_SIZE, checkpoint):\n",
    "    # Here is our training loop!\n",
    "\n",
    "    steps_per_epoch = int(1281167 / BATCH_SIZE)\n",
    "    steps_validation = int(50000 / BATCH_SIZE)\n",
    "\n",
    "    start = time.time()\n",
    "    for train_images, train_labels in train_ds.take(steps_per_epoch):\n",
    "        if step_in_epoch > steps_per_epoch: break\n",
    "        else: step_in_epoch.assign_add(1)\n",
    "\n",
    "        # Peform the training step for this batch\n",
    "        loss, acc = training_step(network, optimizer, train_images, train_labels)\n",
    "        end = time.time()\n",
    "        images_per_second = BATCH_SIZE / (end - start)\n",
    "        print(f\"Finished step {step_in_epoch.numpy()} of {steps_per_epoch} in epoch {i_epoch.numpy()},loss={loss:.3f}, acc={acc:.3f} ({images_per_second:.3f} img/s).\")\n",
    "        start = time.time()\n",
    "\n",
    "    # Save the network after every epoch:\n",
    "    checkpoint.save(\"resnet34/model\")\n",
    "\n",
    "    # Compute the validation accuracy:\n",
    "    mean_accuracy = None\n",
    "    for val_images, val_labels in val_ds.take(steps_validation):\n",
    "        logits = network(val_images)\n",
    "        accuracy = calculate_accuracy(logits, val_labels)\n",
    "        if mean_accuracy is None:\n",
    "            mean_accuracy = accuracy\n",
    "        else:\n",
    "            mean_accuracy += accuracy\n",
    "\n",
    "    mean_accuracy /= steps_validation\n",
    "\n",
    "    print(f\"Validation accuracy after epoch {i_epoch.numpy()}: {mean_accuracy:.4f}.\")\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data_loader(BATCH_SIZE):\n",
    "\n",
    "\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(8)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(8)\n",
    "\n",
    "    print(\"Parameters set, preparing dataloading\")\n",
    "    #########################################################################\n",
    "    # Here's the part where we load datasets:\n",
    "    import json\n",
    "\n",
    "\n",
    "    # What's in this function?  Tune in next week ...\n",
    "    from ilsvrc_dataset import get_datasets\n",
    "\n",
    "\n",
    "    class FakeHvd:\n",
    "\n",
    "        def size(self): return 1\n",
    "\n",
    "        def rank(self): return 0\n",
    "\n",
    "\n",
    "    with open(\"ilsvrc.json\", 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    print(json.dumps(config, indent=4))\n",
    "\n",
    "\n",
    "    config['hvd'] = FakeHvd()\n",
    "    config['data']['batch_size'] = BATCH_SIZE\n",
    "\n",
    "    train_ds, val_ds = get_datasets(config)\n",
    "    print(\"Datasets ready, creating network.\")\n",
    "    #########################################################################\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def main():\n",
    "    BATCH_SIZE = 256\n",
    "    N_EPOCHS = 10\n",
    "\n",
    "    train_ds, val_ds = prepare_data_loader(BATCH_SIZE)\n",
    "\n",
    "\n",
    "    example_images, example_labels = next(iter(train_ds.take(1)))\n",
    "\n",
    "\n",
    "    print(\"Initial Image size: \", example_images.shape)\n",
    "    network = ResNet34()\n",
    "\n",
    "    output = network(example_images)\n",
    "    print(\"output shape:\", output.shape)\n",
    "\n",
    "    print(network.summary())\n",
    "\n",
    "    epoch = tf.Variable(initial_value=tf.constant(0, dtype=tf.dtypes.int64), name='epoch')\n",
    "    step_in_epoch = tf.Variable(\n",
    "        initial_value=tf.constant(0, dtype=tf.dtypes.int64),\n",
    "        name='step_in_epoch')\n",
    "\n",
    "\n",
    "    # We need an optimizer.  Let's use Adam:\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(\n",
    "        network       = network,\n",
    "        optimizer     = optimizer,\n",
    "        epoch         = epoch,\n",
    "        step_in_epoch = step_in_epoch)\n",
    "\n",
    "    # Restore the model, if possible:\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(\"resnet34/\")\n",
    "    if latest_checkpoint:\n",
    "        checkpoint.restore(latest_checkpoint)\n",
    "\n",
    "    while epoch < N_EPOCHS:\n",
    "        train_epoch(epoch, step_in_epoch, train_ds, val_ds, network, optimizer, BATCH_SIZE, checkpoint)\n",
    "        epoch.assign_add(1)\n",
    "        step_in_epoch.assign(0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
